---
# Model Configuration
model:
  name: "Qwen/Qwen2.5-Math-1.5B"
  dtype: "bfloat16"

# Training Configuration
training:
  learning_rate: 7.98e-06        # Standard SFT learning rate (range: 1e-5 to 5e-5)
  weight_decay: 0.01
  batch_size: 512              # Optimal for H100 80GB (~28GB usage)
  num_epochs: 1
  gradient_accumulation_steps: 2  # Effective batch = 80
  warmup_steps: 100           # 10-20% of total training steps for stability
  max_batches: 10000          # Use full dataset (MATH has ~7500 train examples)
  eval_every: 1000             # Evaluate every 500 batches
  device: "cuda:0"

# Evaluation Configuration
evaluation:
  device: "cuda:1"
  batch_size: 320             # Large batch for fast eval (~40GB usage)
  num_eval_samples: 10000     # Use full test dataset (MATH has ~5000 test examples)
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  
# Generation Parameters
generation:
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  stop_sequences: ["</answer>"]
  include_stop_str: true

# Data Configuration
data:
  prompt_file: "prompts/rl_zero.prompt"
  train_val_split: 0.8
  
# Checkpointing
checkpointing:
  output_dir: "results/checkpoints"
  queue_maxsize: 2
  temp_dir: "/tmp/qwen_sft_checkpoints"
  
# Logging
logging:
  wandb_project: "math-sft"
  wandb_entity: null
  log_every: 1                # Log EVERY batch to W&B
