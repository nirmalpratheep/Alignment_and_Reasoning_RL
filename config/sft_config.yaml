---
# Model Configuration
model:
  name: "Qwen/Qwen2.5-Math-1.5B"
  dtype: "bfloat16"

# Training Configuration
training:
  learning_rate: 2.0e-5
  weight_decay: 0.01
  batch_size: 32              # Optimal for H100 80GB (~28GB usage)
  num_epochs: 1
  gradient_accumulation_steps: 4  # Effective batch = 64
  warmup_steps: 10            # Reduced for small test run
  max_batches: 500             # Small test run (20 batches)
  eval_every: 10              # Evaluate after 10 batches
  device: "cuda:0"

# Evaluation Configuration
evaluation:
  device: "cuda:1"
  batch_size: 256             # Large batch for fast eval (~40GB usage)
  num_eval_samples: 50        # Reduced for quick test eval
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  
# Generation Parameters
generation:
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  stop_sequences: ["</answer>"]
  include_stop_str: true

# Data Configuration
data:
  prompt_file: "prompts/rl_zero.prompt"
  train_val_split: 0.8
  
# Checkpointing
checkpointing:
  output_dir: "results/checkpoints"
  queue_maxsize: 2
  temp_dir: "/tmp/qwen_sft_checkpoints"
  
# Logging
logging:
  wandb_project: "math-sft"
  wandb_entity: null
  log_every: 1                # Log EVERY batch to W&B
