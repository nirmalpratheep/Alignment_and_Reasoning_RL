---
# Model Configuration
model:
  name: "Qwen/Qwen2.5-Math-1.5B"
  dtype: "bfloat16"

# Training Configuration
training:
  learning_rate: 2.0e-5
  weight_decay: 0.01
  batch_size: 1
  num_epochs: 1
  gradient_accumulation_steps: 4
  warmup_steps: 100
  max_batches: 50
  eval_every: 500
  device: "cuda:0"

# Evaluation Configuration
evaluation:
  device: "cuda:1"
  batch_size: 128
  num_eval_samples: 100
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  
# Generation Parameters
generation:
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  stop_sequences: ["</answer>"]
  include_stop_str: true

# Data Configuration
data:
  prompt_file: "prompts/rl_zero.prompt"
  train_val_split: 0.8
  
# Checkpointing
checkpointing:
  output_dir: "results/checkpoints"
  queue_maxsize: 2
  temp_dir: "/tmp/qwen_sft_checkpoints"
  
# Logging
logging:
  wandb_project: "math-sft"
  wandb_entity: null
  log_every: 10
