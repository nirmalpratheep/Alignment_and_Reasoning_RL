---
# FSDP Configuration for 2Ã— H100 GPUs
fsdp:
  world_size: 2
  sharding_strategy: "FULL_SHARD"
  auto_wrap_policy: "transformer"
  mixed_precision: "bfloat16"
  forward_prefetch: true
  limit_all_gathers: true
  sync_module_states: true

# Model Configuration
model:
  name: "checkpoint/Qwen2.5-Nirmal-Math-1.5B-Instruct"  # SFT checkpoint
  dtype: "bfloat16"
  trust_remote_code: true
  # attn_implementation: "flash_attention_2"  # H100 optimization (requires: pip install flash-attn)
  torch_compile: true
  compile_mode: "max-autotune"

# Training Configuration
training:
  learning_rate: 1.0e-5
  weight_decay: 0.0
  betas: [0.9, 0.95]
  batch_size_per_gpu: 128        # train_batch_size=256 / 2 GPUs = 128 per GPU
  gradient_accumulation_steps: 128  # microbatch size is 2, will fit on H100
  max_steps: 200
  eval_every: 50
  max_grad_norm: 1.0
  
# GRPO-Specific Parameters
grpo:
  group_size: 8                    # K completions per prompt (from reference)
  temperature: 1.0                 # sampling_temperature
  max_tokens: 1024                 # sampling_max_tokens
  min_tokens: 4                    # sampling_min_tokens (disallow empty responses)
  kl_coef: 0.0                     # KL penalty coefficient
  advantage_eps: 1.0e-6            # From reference
  use_std_normalization: true      # From reference
  loss_type: "reinforce_with_baseline"  # From reference (vs no_baseline or grpo_clip)
  gpu_memory_utilization: 0.85     # For vLLM generation
  
# Data Configuration
data:
  prompt_file: "prompts/rl_zero.prompt"
  num_train_samples: null          # Use all training data
  
# Evaluation Configuration
evaluation:
  batch_size: 320
  num_samples: 1000                # Samples to evaluate (for speed)
  temperature: 1.0
  top_p: 1.0
  max_tokens: 1024
  
# Checkpointing
checkpointing:
  output_dir: "results/grpo_fsdp_checkpoints"
  save_total_limit: 3
  
# Logging
logging:
  wandb_project: "math-grpo-h100"
  wandb_entity: null
  log_every_steps: 1               # Log every step
